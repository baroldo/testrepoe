import shap

# 1. Initialize SHAP explainer (TreeExplainer is optimized for tree models)
explainer = shap.TreeExplainer(rf_model)

# 2. Select your group of customers (e.g., high churn risk, or by segment)
group = X_test[X_test['segment'] == 'High Value']  # or whatever filter you want

# 3. Calculate SHAP values for the group
shap_values = explainer.shap_values(group)

# 4. For classification models, shap_values is a list (one for each class)
# Use the one for the "churn" class (typically index 1)
shap_values_churn = shap_values[1]

# 5. Summarize average impact across the group
shap.summary_plot(shap_values_churn, group)  # beeswarm plot

edition 2:

# 📦 Import required libraries
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 🔄 Step 1: Load your dataset
# Replace with your actual dataset path or DataFrame
df = pd.read_csv("your_data.csv")  # Must include a 'churn' column

# 🎯 Step 2: Define features (X) and target (y)
X = df.drop(columns="churn")
y = df["churn"]

# 🧪 Step 3: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 🌲 Step 4: Train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 🔍 Step 5: Set up SHAP for explanation
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)  # Class 1 = churn

# 🧠 Step 6A: Explain a single customer prediction
shap.initjs()
customer_idx = 10  # Choose any index in the test set
shap.force_plot(
    explainer.expected_value[1],         # Base churn risk
    shap_values[1][customer_idx],        # SHAP values for this customer
    X_test.iloc[customer_idx],           # Actual feature values for this customer
)

# 🧠 Step 6B: Explain global feature importance across all customers
shap.summary_plot(shap_values[1], X_test)

# 🧠 Step 6C: (Optional) Segment-specific SHAP explanation
# Example: Focus on low engagement customers if such a feature exists
if "engagement_score" in X_test.columns:
    segment = X_test[X_test["engagement_score"] < 0.3]
    segment_shap_values = explainer.shap_values(segment)
    shap.summary_plot(segment_shap_values[1], segment)



3. change for index:

import shap
import pandas as pd

# -----------------------------
# STEP 1: Clean and prepare data
# -----------------------------

def clean_features_for_model(X):
    X_clean = X.copy()
    
    # Convert bool columns to float
    bool_cols = X_clean.select_dtypes(bool).columns
    X_clean[bool_cols] = X_clean[bool_cols].astype(float)
    
    # Attempt to convert any object columns to numeric (e.g., from encoded dates or categories)
    X_clean = X_clean.apply(pd.to_numeric, errors='coerce')
    
    # Fill any remaining NaNs with column means
    X_clean = X_clean.fillna(X_clean.mean(numeric_only=True))
    
    return X_clean

# Clean training data and full data
X_clean = clean_features_for_model(X)
X_train_clean = clean_features_for_model(X_train)

# -----------------------------
# STEP 2: Get customer row
# -----------------------------

customer_id = "abc145"

# If customer ID is in a column
customer_row = X_clean[X_clean["customer_id"] == customer_id]

# Or if it's in the index:
# customer_row = X_clean.loc[[customer_id]]

# Ensure only one row returned
assert len(customer_row) == 1, f"Customer {customer_id} not found or not unique."

# -----------------------------
# STEP 3: Run SHAP
# -----------------------------

# Create SHAP explainer
explainer = shap.Explainer(model, X_train_clean)

# Get SHAP values for customer
shap_values = explainer(customer_row)

# -----------------------------
# STEP 4: Plot SHAP explanation
# -----------------------------

# Show waterfall plot for the single customer
shap.plots.waterfall(shap_values[0])



test 3: shap after reset

import shap

# Step 1: Select the row where customer_number == 'abl209'
row = df_model[df_model['customer_number'] == 'abl209']

# Step 2: Drop the customer_number (if it's not part of training features)
X_row = row.drop(columns=['customer_number'])

# Step 3: Create SHAP TreeExplainer and get SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_row)

# Step 4: Generate force plot (correct order for SHAP v0.20+)
shap.force_plot(
    base_value=explainer.expected_value,
    shap_values=shap_values,
    features=X_row,
    matplotlib=True
)
