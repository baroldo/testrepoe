man mom import shap

# 1. Initialize SHAP explainer (TreeExplainer is optimized for tree models)
explainer = shap.TreeExplainer(rf_model)

# 2. Select your group of customers (e.g., high churn risk, or by segment)
group = X_test[X_test['segment'] == 'High Value']  # or whatever filter you want

# 3. Calculate SHAP values for the group
shap_values = explainer.shap_values(group)

# 4. For classification models, shap_values is a list (one for each class)
# Use the one for the "churn" class (typically index 1)
shap_values_churn = shap_values[1]

# 5. Summarize average impact across the group
shap.summary_plot(shap_values_churn, group)  # beeswarm plot

edition 2:

# 📦 Import required libraries
import pandas as pd
import numpy as np
import shap
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 🔄 Step 1: Load your dataset
# Replace with your actual dataset path or DataFrame
df = pd.read_csv("your_data.csv")  # Must include a 'churn' column

# 🎯 Step 2: Define features (X) and target (y)
X = df.drop(columns="churn")
y = df["churn"]

# 🧪 Step 3: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 🌲 Step 4: Train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 🔍 Step 5: Set up SHAP for explanation
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)  # Class 1 = churn

# 🧠 Step 6A: Explain a single customer prediction
shap.initjs()
customer_idx = 10  # Choose any index in the test set
shap.force_plot(
    explainer.expected_value[1],         # Base churn risk
    shap_values[1][customer_idx],        # SHAP values for this customer
    X_test.iloc[customer_idx],           # Actual feature values for this customer
)

# 🧠 Step 6B: Explain global feature importance across all customers
shap.summary_plot(shap_values[1], X_test)

# 🧠 Step 6C: (Optional) Segment-specific SHAP explanation
# Example: Focus on low engagement customers if such a feature exists
if "engagement_score" in X_test.columns:
    segment = X_test[X_test["engagement_score"] < 0.3]
    segment_shap_values = explainer.shap_values(segment)
    shap.summary_plot(segment_shap_values[1], segment)



3. change for index:

import shap
import pandas as pd

# -----------------------------
# STEP 1: Clean and prepare data
# -----------------------------

def clean_features_for_model(X):
    X_clean = X.copy()
    
    # Convert bool columns to float
    bool_cols = X_clean.select_dtypes(bool).columns
    X_clean[bool_cols] = X_clean[bool_cols].astype(float)
    
    # Attempt to convert any object columns to numeric (e.g., from encoded dates or categories)
    X_clean = X_clean.apply(pd.to_numeric, errors='coerce')
    
    # Fill any remaining NaNs with column means
    X_clean = X_clean.fillna(X_clean.mean(numeric_only=True))
    
    return X_clean

# Clean training data and full data
X_clean = clean_features_for_model(X)
X_train_clean = clean_features_for_model(X_train)

# -----------------------------
# STEP 2: Get customer row
# -----------------------------

customer_id = "abc145"

# If customer ID is in a column
customer_row = X_clean[X_clean["customer_id"] == customer_id]

# Or if it's in the index:
# customer_row = X_clean.loc[[customer_id]]

# Ensure only one row returned
assert len(customer_row) == 1, f"Customer {customer_id} not found or not unique."

# -----------------------------
# STEP 3: Run SHAP
# -----------------------------

# Create SHAP explainer
explainer = shap.Explainer(model, X_train_clean)

# Get SHAP values for customer
shap_values = explainer(customer_row)

# -----------------------------
# STEP 4: Plot SHAP explanation
# -----------------------------

# Show waterfall plot for the single customer
shap.plots.waterfall(shap_values[0])



test 3: shap after reset

import shap

# Step 1: Select the row where customer_number == 'abl209'
row = df_model[df_model['customer_number'] == 'abl209']

# Step 2: Drop the customer_number (if it's not part of training features)
X_row = row.drop(columns=['customer_number'])

# Step 3: Create SHAP TreeExplainer and get SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_row)

# Step 4: Generate force plot (correct order for SHAP v0.20+)
shap.force_plot(
    base_value=explainer.expected_value,
    shap_values=shap_values,
    features=X_row,
    matplotlib=True
)


update 4: waterfall plot
import shap

# Drop identifier column
X_row = df_model[df_model['customer_number'] == 'abl209'].drop(columns=['customer_number'])

# Force tree explainer instead of generic explainer
explainer = shap.TreeExplainer(model)

# Get SHAP values
shap_values = explainer.shap_values(X_row)

# If shap_values is a list (multi-output), pick the correct output
if isinstance(shap_values, list):
    print("Multi-output detected, using first output")
    shap_values = shap_values[0]

# Create a dummy Explanation object manually if needed
explanation = shap.Explanation(
    values=shap_values[0],
    base_values=explainer.expected_value,
    data=X_row.iloc[0],
    feature_names=X_row.columns
)

# Plot waterfall
shap.plots.waterfall(explanation)


sample data:
import pandas as pd
import shap
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# 1. Load data
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target

# 2. Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# 3. Fit a RandomForestClassifier
model = RandomForestClassifier()
model.fit(X_train, y_train)

# 4. Create a SHAP explainer
explainer = shap.Explainer(model, X_train)

# 5. Compute SHAP values for a single instance (index 0)
shap_values = explainer(X_test.iloc[[0]])

# 6. Get predicted class
pred_class_index = model.predict(X_test.iloc[[0]])[0]

# 7. Plot force plot correctly for that class
shap.initjs()
shap.force_plot(
    base_value=shap_values[0].base_values[pred_class_index],
    shap_values=shap_values[0].values[pred_class_index],
    features=shap_values[0].data,
    feature_names=X.columns
)

eli5 library:

import eli5
from eli5.sklearn import PermutationImportance
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer

# Load and split dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Train model
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Explain one prediction (e.g. row 0 of test set)
import eli5
eli5.show_prediction(model, X_test.iloc[0], feature_names=X.columns.tolist())