sample 3: for assigning each customer to its own group if it doesnt match an exisitng one
import pandas as pd
import re
from rapidfuzz import fuzz

# -- Example starting DataFrame --
# df = pd.read_csv("your_file.csv")
# df['customer_name'] = ...

# -- Clean customer names --
suffixes = ['pty', 'ltd', 'limited', 'group', 'co', 'inc', 'corp', 'company', 'australia', 'australian']
pattern = r'\b(?:' + '|'.join(suffixes) + r')\b'

def clean_name(name):
    name = name.lower()
    name = re.sub(pattern, '', name)
    name = re.sub(r'[^a-z0-9 ]+', '', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name

df['cleaned_name'] = df['customer_name'].fillna('').apply(clean_name)

# -- Create fuzzy groups --
group_names = []
group_ids = []
group_counter = 0
threshold = 90  # similarity threshold

for name in df['cleaned_name']:
    assigned = False
    for idx, group in enumerate(group_names):
        if fuzz.token_sort_ratio(name, group) >= threshold:
            group_ids.append(idx)
            assigned = True
            break
    if not assigned:
        group_names.append(name)
        group_ids.append(group_counter)
        group_counter += 1

df['name_group_id'] = group_ids
df['grouped_name'] = [group_names[i] for i in group_ids]


group_counts = df.groupby('grouped_name')['customer_id'].nunique().reset_index()
group_counts.columns = ['grouped_name', 'unique_customer_count']
df = df.merge(group_counts, on='grouped_name', how='left')






# sample 2.1: refined groupings
import pandas as pd
import numpy as np
import re

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_distances
from fuzzywuzzy import process

# --------------------------------------
# 1. DEFINE COMMON SUFFIXES TO REMOVE
# --------------------------------------
common_suffixes = [
    'pty', 'ltd', 'limited', 'group', 'co', 'inc', 'corp', 'company', 'australia', 'australian'
]

# Build regex pattern from list
suffix_pattern = r'\b(?:' + '|'.join(common_suffixes) + r')\b'

# --------------------------------------
# 2. CLEAN CUSTOMER NAMES
# --------------------------------------
def clean_name(name):
    name = name.lower()
    name = re.sub(suffix_pattern, '', name)  # remove unwanted suffixes
    name = re.sub(r'[^a-z0-9 ]', '', name)   # remove punctuation
    name = re.sub(r'\s+', ' ', name).strip() # collapse whitespace
    return name

df['cleaned_name'] = df['customer_name'].fillna('').apply(clean_name)

# --------------------------------------
# 3. TF-IDF ENCODING
# --------------------------------------
vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 5))
X = vectorizer.fit_transform(df['cleaned_name'])

# --------------------------------------
# 4. DBSCAN CLUSTERING
# --------------------------------------
clustering = DBSCAN(eps=0.2, min_samples=2, metric='cosine', n_jobs=-1)
df['name_cluster'] = clustering.fit_predict(X)

# --------------------------------------
# 5. FIND CLUSTER REPRESENTATIVE (MEDOID)
# --------------------------------------
def get_cluster_medoid_name(X_sparse, labels, names):
    cluster_labels = np.unique(labels[labels != -1])
    rep_names = {}

    for clust in cluster_labels:
        indices = np.where(labels == clust)[0]
        vectors = X_sparse[indices].toarray()  # convert sparse to dense
        cluster_center = vectors.mean(axis=0)
        dists = cosine_distances(vectors, cluster_center.reshape(1, -1))
        medoid_idx = indices[np.argmin(dists)]
        rep_names[clust] = names.iloc[medoid_idx]
    
    return rep_names

rep_names = get_cluster_medoid_name(X, clustering.labels_, df['customer_name'])
df['grouped_name'] = df['name_cluster'].map(rep_names)
df['grouped_name'] = df['grouped_name'].fillna(df['customer_name'])  # fallback

# --------------------------------------
# 6. FUZZY MATCH UNCLUSTERED NAMES
# --------------------------------------
ungrouped_mask = df['name_cluster'] == -1
unmatched = df.loc[ungrouped_mask, 'cleaned_name']
known_groups = df.loc[~ungrouped_mask, 'grouped_name'].unique()

df.loc[ungrouped_mask, 'grouped_name'] = unmatched.apply(
    lambda name: process.extractOne(name, known_groups)[0] if name else name
)

# --------------------------------------
# 7. COUNT UNIQUE CUSTOMER IDS PER GROUP
# --------------------------------------
group_counts = df.groupby('grouped_name')['customer_id'].nunique().reset_index()
group_counts.columns = ['grouped_name', 'unique_customer_count']

# --------------------------------------
# 8. MERGE COUNTS BACK TO ORIGINAL DATAFRAME
# --------------------------------------
df = df.merge(group_counts, on='grouped_name', how='left')

# --------------------------------------
# 9. OPTIONAL PREVIEW
# --------------------------------------
print(df[['customer_id', 'customer_name', 'grouped_name', 'unique_customer_count']].head(10))




















import pandas as pd
import re
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import DBSCAN

# --------------------------
# 1. Rank Common Words
# --------------------------

def tokenize(name):
    name = str(name).lower()
    name = re.sub(r'[^a-z0-9\s]', '', name)
    return name.split()

# Tokenize and count
word_list = df['customer_name'].dropna().apply(tokenize)
all_words = [word for sublist in word_list for word in sublist]
word_freq = Counter(all_words)

# View top common words
word_freq_df = pd.DataFrame(word_freq.items(), columns=['word', 'count']).sort_values(by='count', ascending=False)
print(word_freq_df.head(30))  # Optional: inspect this to choose custom stopwords

# --------------------------
# 2. Define Stopwords + Clean Names
# --------------------------

COMMON_WORDS_TO_REMOVE = [
    'pty', 'ltd', 'limited', 'inc', 'llc', 'group', 'company', 'business', 'co',
    'university', 'corporation', 'plc', 'trust', 'foundation', 'association'
]

def clean_name(name):
    name = str(name).lower()
    name = re.sub(r'[^a-z0-9\s]', '', name)  # remove punctuation
    tokens = name.split()
    tokens = [t for t in tokens if t not in COMMON_WORDS_TO_REMOVE]
    return ' '.join(tokens)

df['cleaned_name'] = df['customer_name'].apply(clean_name)

# --------------------------
# 3. TF-IDF Vectorization
# --------------------------

vectorizer = TfidfVectorizer(analyzer='char_wb', ngram_range=(3, 4))
X = vectorizer.fit_transform(df['cleaned_name'])

# --------------------------
# 4. Clustering with DBSCAN
# --------------------------

clustering = DBSCAN(eps=0.3, min_samples=2, metric='cosine', n_jobs=-1)
df['name_cluster'] = clustering.fit_predict(X)

# --------------------------
# 5. Assign Group Label from Most Common Name
# --------------------------

# For each cluster, assign the most common original name as the label
rep_names = df[df['name_cluster'] != -1].groupby('name_cluster')['customer_name'].agg(lambda x: x.mode().iloc[0])
df['grouped_name'] = df['name_cluster'].map(rep_names)

# Optional: Fill noise points (cluster -1) with their original name
df['grouped_name'] = df['grouped_name'].fillna(df['customer_name'])

# --------------------------
# âœ… Done
# --------------------------

# Preview result
df[['customer_name', 'cleaned_name', 'name_cluster', 'grouped_name']].head(10)
